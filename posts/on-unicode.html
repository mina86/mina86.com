<!-- subject: On Unicode -->
<!-- date: 2015-10-25 02:11:11 -->
<!-- tags: unicode, utf-8, utf-16, cjk -->
<!-- categories: Articles, English, Techblog -->

<p>There is <em>a lot</em> of misconceptions about Unicode.  Most are
  there because people assume what they know about ASCII or ISO-8859-*
  is true about Unicode.  They are usually harmless but they tend to
  creep into minds of people who work with text which leads to badly
  designed software and technical decisions made based on false
  information.

<p>Without further ado, hereâ€™s a few facts about Unicode that might
  surprise you.

<!-- EXCERPT -->

<style>
span.hisur { text-decoration: overline; background: #FFD }
span.losur { text-decoration: underline; background: #DFF }
span.xxsur { text-decoration: line-through; background: #DDD }
span.chsur { background: #FDE }
span.clsur { background: #EDF }
span.udesc { font-variant: all-small-caps }
</style>


<h3>UTF-16 is <em>not</em> a ï¬xed-width encoding</h3>

<p>Unicode deï¬nes 17 <dfn>planes</dfn> (most famous being plane zero,
  the Basic Multilingual Plane or BMP).  Each plane consists of
  65â€Š536 <dfn>code points</dfn>.  Quick multiplication unveils
  staggering number of 1â€Š114â€Š112 entries.  It quickly becomes obvious
  that 16 bits, which is the size of a single UTF-16 <dfn>code
  unit</dfn>, arenâ€™t enough to identify each code point uniquely.

<p>To solve that problem, a somehow awkward concept of surrogate pairs
  has been introduced.  (On unrelated note, is it just me who ï¬nds
  spelling of â€˜awkwardâ€™ so <em>meta</em>?) Anyway, 2048 code points
  have been carved out to make room for high and low surrogates.  In
  UTF-16, a high surrogate followed by a low surrogateâ€”four octets
  totalâ€”encodes a single code point outside of BMP.

<p>The encoding method is relatively simple.  For example, to
  represent U+1F574: <span class=udesc>man in business suit
  levitating</span> (ğŸ•´â€‰â€“â€‰does your browser support it yet?)
  one would:

<ol>
  <li>
    <p>Subtract 10000â‚â‚† from the code point to produce a 20-bit
      number.
    <p>1F574â‚â‚† - 10000â‚â‚† = F574â‚â‚†
      = <span class=hisur>0000111101</span><span class=losur>0101110100</span>â‚‚
  <li>
    <p>Add D800â‚â‚† to the ten most signiï¬cant bits of that
      numberâ€”thatâ€™s the high surrogate.
    <p>D800â‚â‚† + <span class=hisur>0000111101</span>â‚‚
      = <span class=chsur>110110</span><span class=xxsur>0000000000</span>â‚‚
      + <span class=hisur>0000111101</span>â‚‚
      = <span class=chsur>110110</span><span class=hisur>0000111101</span>â‚‚
      = D83Dâ‚â‚†
  <li>
    <p>Add DC00â‚â‚† to the ten least signiï¬cant bits of the same
      numberâ€”thatâ€™s the low surrogate.
    <p>DC00â‚â‚† + <span class=losur>0101110100</span>â‚‚
      = <span class=clsur>110111</span><span class=xxsur>0000000000</span>â‚‚
      + <span class=losur>0101110100</span>â‚‚
      = <span class=clsur>110111</span><span class=losur>0101110100</span>â‚‚
      = DD74â‚â‚†
  <li>
    <p>Output the high surrogate followed by low surrogate.
    <p>UTF-16 encoding of U+1F574 is U+D83D U+DD74.
</ol>


<h3>Case change is not reversible (and changes length)</h3>

<p>An observant reader, or one using software with subpar Unicode
  support, might notice that within this article, sequences of
  a letter â€˜fâ€™ followed by a letter â€˜iâ€™ are replaced by
  U+FB01: <span class=udesc>Latin small ligature fi</span>.  Despite
  having â€˜smallâ€™ in its description, the character doesnâ€™t have
  a corresponding â€˜capitalâ€™ counterpart.  This implies that
  <code>lc(uc(<i>deï¬ne</i>)) = lc(<i>DEFINE</i>) = <i>define</i></code>
  which code point for code point is different
  than <code><i>deï¬ne</i></code>.

<p>Similarly, even though capital sharp s is deï¬ned, it is rarely ever
  used and instead a proper, according to German orthography, upper
  case form of â€˜ÃŸâ€™ is â€˜SSâ€™.

<p>In both examples, changing case not only is not reversible but also
  increases length of a string (one code point becomes two).

<p>But thatâ€™s not all!  It gets even weirder.  Below is a table of
  some corner cases.  My experiments show that Firefox handles all
  cases correctly while Chrome and Opera fail to properly capitalise
  â€˜ï¬lmâ€™.

<table>
  <thead>
    <tr><th>Operation<th>Expected<th>Browserâ€™s handling
    <tr><th colspan=3>Notes
  <tbody style="border-top:1px dotted #000;border-bottom:1px dotted #000">
    <tr>
      <td scope=row>uc(â€˜deï¬neâ€™)
      <td>DEFINE
      <td style="text-transform:uppercase">deï¬ne
    <tr>
      <td scope=row>uc(â€˜<span lang=de>heiÃŸ</span>â€™)
      <td lang=de>HEISS
      <td style="text-transform:uppercase" lang=de>heiÃŸ
    <tr>
      <td scope=row>tc(â€˜ï¬lmâ€™)
      <td>Film
      <td style="text-transform:capitalize">ï¬lm
    <tr>
      <td colspan=3 style=padding-left:2em>Ligatures and digraphs
        often need to be converted into separate characters.
  <tbody style="border-top:1px dotted #000;border-bottom:1px dotted #000">
    <tr>
      <td scope=row>tc(â€˜<span lang=hr>ÇŒeÅ¾an</span>â€™)
      <td lang=hr>Ç‹eÅ¾an
      <td lang=hr style="text-transform:capitalize">ÇŒeÅ¾an
    <tr>
      <td scope=row>lc(â€˜â…§â€™)
      <td>â…·
      <td style="text-transform:lowercase">â…§
     <tr>
      <td colspan=3 style=padding-left:2em>Some ligatures and digraphs
        have corresponding characters in desired case.
  <tbody style="border-top:1px dotted #000;border-bottom:1px dotted #000">
   <tr>
      <td scope=row>lc(â€˜<span lang=el>ÎŒÎ£ÎŸÎ£</span>â€™)
      <td lang=el>ÏŒÏƒÎ¿Ï‚</td>
      <td lang=el style="text-transform:lowercase">ÎŒÎ£ÎŸÎ£
    <tr>
      <td colspan=3 style=padding-left:2em>Lower case sigma is â€˜Ïƒâ€™ in
        the middle but â€˜Ï‚â€™ at the end of a word.
    <tr>
      <td scope=row>uc(â€˜<span lang=tr>istanbul</span>â€™)
      <td lang=tr>Ä°STANBUL</td>
      <td lang=tr style="text-transform:uppercase">istanbul
    <tr>
      <td scope=row>lc(â€˜<span lang=tr>IRMAK</span>â€™)
      <td lang=tr>Ä±rmak</td>
      <td lang=tr style="text-transform:lowercase">IRMAK
    <tr>
      <td colspan=3 style=padding-left:2em>Turkish has a dot-less
        (a.k.a. closed) and dotted â€˜iâ€™.  Capital â€˜iâ€™ includes a tittle
        while lower case â€˜Iâ€™ dosenâ€™t.
</table>


<h3>Single letter may map to multiple code points</h3>

<p>Above examples show that concepts of a letter or a character may be
  blurry and confusing.  Is aforementioned â€˜ÃŸâ€™ a letter or just
  a fancy way of writing â€˜ssâ€™ or maybe â€˜szâ€™?  What of ligatures and
  digraphs?  But at least everyone can agree that â€˜Ã©â€™ is a single
  letter and <span class=udesc>Latin small letter e with acute</span>
  is its name, right?  Here it is again: â€˜e&#769;â€™, except this time
  itâ€™s a regular letter e followed by
  U+0301: <span class=udesc>combining acute accent</span>, i.e. â€˜eâ—ŒÌâ€™.

<p>The former, single-code-point representation, is
  called <dfn>precomposed</dfn> (or <dfn>composed</dfn>) while the
  latter, using combining characters, is called <dfn>decomposed</dfn>.
  Whatâ€™s important is that both sequences are <dfn>canonically
  equivalent</dfn> and proper Unicode implementations should treat them
  identically.  They should be indistinguishable based on rendering or
  behaviour (e.g. when selecting text).

<p>In addition to the above, Polish â€˜Ä…â€™ â‰ˆ â€˜aâ—ŒÌ¨â€™, Korean â€˜í•œâ€™ â‰ˆ â€˜ã…ã…ã„´â€™,
  â€˜â„¦â€™ (U+2126: <span class=udesc>Ohm sign</span>) â‰ˆ â€˜Î©â€™
  (U+03A9: <span class=udesc>Greek capital letter omega</span>), Hebrew
  â€˜ï¬«â€â€™ â‰ˆ â€˜×©â€â—Œâ€×‚â€™ and more.

<p>Based on canonical equivalence, Unicode deï¬nes a Normalised Form
  C (NFC) and Normalised Form D (NFD).  The former often uses
  precomposed while the latter uses decomposed representation of
  characters.

<p>Oh, and by the way, aforementioned <span class=udesc>Ohm
  sign</span> is <dfn>a singleton</dfn> which means that it disappears
  from the text after any kind of normalisation.  Thereâ€™s a bunch of
  those.

<p>â€˜Converting to NFCâ€™, a hopeful programmer will say, â€˜guarantees
  that a single letter maps to no more than one code point!â€™ Alas, noâ€¦
  For example, there is no Unicode character for â€˜á¸Ì‡â€™ (i.e. letter
  d with dot above and below).  No matter what form is used, the
  character must take more than one code point.

<p>NFC is not even guaranteed to be the shortest representation of a given string.  Weâ€™ve already seen that
  â€˜<span dir=ltr>ï¬«</span>â€™ is canonically equivalent to
  â€˜<span dir=ltr>×©â—Œ×‚</span>â€™ but whatâ€™s more interesting is that
  the latter is in NFC.  Yes, even thought precomposed
  character exists, decomposed representation is in NFC.  In fact, for
  <span class=udesc>Hebrew letter shin with sin dot</span> normalised
  forms C and D are the same.

<p>As to not leave an impression that NFC is the odd ball here, even
  though NFD usually decomposes precomposed characters, it not always
  does so. â€˜Ã¸â€™ (U+00F8: <span class=udesc>Latin small letter o with
  stroke</span>) is in its NFD (as a single code-point) even though
  a decomposed representation with a combining stroke also exists.

<p>Thereâ€™s also a <dfn>compatibility equivalence</dfn> which can be
  thought of as covering â€˜meaningâ€™ of strings.  For example â€˜ï¬â€™
  (U+FB01: <span class=udesc>Latin small ligature fi</span>) means the
  same thing as â€˜f + iâ€™, â€˜Ç† âˆ¼ d + z + â—ŒÌŒâ€™ etc.  This is a bit simpliï¬ed
  view though since â€˜5Â²â€™ has a distinct meaning from â€˜52â€™ yet the
  sequences are in the same compatibly equivalence class.


<h3>UTF-8 is better for CJK than UTF-16</h3>

<p>An argument sometimes put in favour of UTF-16 (over UTF-8) is that
  it is better for far eastern scripts.  For majority of Chinese,
  Japanese and Korean (CJK) ideographs UTF-16 takes just two octets
  while UTF-8 takes three.  Clearly, Asia should abandon UTF-8 and use
  UTF-16 then, right?

<table>
  <thead>
    <tr><th rowspan=2>Block<br>(Range)<th colspan=2>Octets used by
    <tr><th>UTF-8<th>UTF-16
  <tbody>
    <tr><td>CJK Uniï¬ed Ideographs Extension A<br>(U+3400â€“U+4DBF)  <td>3<td>2
    <tr><td>CJK Uniï¬ed Ideographs            <br>(U+4E00â€“U+9FFF)  <td>3<td>2
    <tr><td>CJK Uniï¬ed Ideographs Extension B<br>(U+20000â€“U+2A6DF)<td>4<td>4
    <tr><td>CJK Uniï¬ed Ideographs Extension C<br>(U+2A700â€“U+2B73F)<td>4<td>4
    <tr><td>CJK Uniï¬ed Ideographs Extension D<br>(U+2B740â€“U+2B81F)<td>4<td>4
</table>

<p>Alas, the devil, as he often does, lays in the details, namely in
  the fact that in most cases the CJK text is accompanied by markup
  which uses US-ASCII characters.  Since those need only one UTF-8 code
  unit, it often more than makes up for octets â€˜lostâ€™ when encoding
  ideographs.

<p>To see just how big of a role this effect plays in real life
  I looked at a bunch of websites popular in China, Japan and South
  Korea and compared their size (in kibibytes) when different
  encodings were used.  The results are as follows:

<table>
  <thead>
    <tr><th scope=col>Page<th scope=col>UTF-8<th scope=col>UTF-16<th scope=col>Increase
  <tbody>
    <tr><td>baidu.com    <td class=rht>   91<td class=rht>  181<td class=rht>100%
    <tr><td>tmall.com    <td class=rht>   46<td class=rht>   90<td class=rht> 97%
    <tr><td>daum.net     <td class=rht>  155<td class=rht>  300<td class=rht> 94%
    <tr><td>taobao.com   <td class=rht>   40<td class=rht>   76<td class=rht> 93%
    <tr><td>amacon.co.jp <td class=rht>  216<td class=rht>  413<td class=rht> 91%
    <tr><td>rakuten.co.jp<td class=rht>  291<td class=rht>  548<td class=rht> 88%
    <tr><td>gmarket.co.kr<td class=rht>   71<td class=rht>  133<td class=rht> 88%
    <tr><td>weibo.com    <td class=rht>    6<td class=rht>   11<td class=rht> 86%
    <tr><td>yahoo.co.jp  <td class=rht>   18<td class=rht>   34<td class=rht> 85%
    <tr><td>naver.com    <td class=rht>   80<td class=rht>  147<td class=rht> 83%
    <tr><td>ppomppu.co.kr<td class=rht>  142<td class=rht>  259<td class=rht> 83%
    <tr><td>zn.wiki/Japan<td class=rht>  938<td class=rht>1â€‰690<td class=rht> 80%
    <tr><td>kr.wiki/Japan<td class=rht>  782<td class=rht>1â€‰370<td class=rht> 75%
    <tr><td>zn.wiki/Korea<td class=rht>   67<td class=rht>  116<td class=rht> 73%
    <tr><td>fc2.com      <td class=rht>   35<td class=rht>   60<td class=rht> 72%
    <tr><td>jp.wiki/Korea<td class=rht>  123<td class=rht>  211<td class=rht> 71%
    <tr><td>kr.wiki/Korea<td class=rht>  180<td class=rht>  303<td class=rht> 69%
    <tr><td>jp.wiki/Japan<td class=rht>1â€‰012<td class=rht>1â€‰616<td class=rht> 60%
</table>

<p>Yes, in the worst case, baidu.comâ€™s size nearly <em>doubled</em>
  when using UTF-16.

<p>If size is a concern, one might decide to use a dedicated encoding
  such us Shift_JIS, version of EUC or GB2312.  And indeed, some sites
  did that, but even then advantage over UTF-8 was minimal:

<table>
  <thead>
    <tr>
      <th scope=col>Page
      <th scope=col>Original [KiB]
      <th scope=col>UTF-8 [KiB]
      <th scope=col>Increase
  <tbody>
    <tr><td>ppomppu.co.kr (euc-kr)   <td class=rht>136<td class=rht>142<td class=rht>4.5%
    <tr><td>weibo.com     (gb2312)   <td class=rht>  5<td class=rht>  6<td class=rht>3.6%
    <tr><td>gmarket.co.kr (euc-kr)   <td class=rht> 69<td class=rht> 71<td class=rht>3.2%
    <tr><td>rakuten.co.jp (euc-jp)   <td class=rht>283<td class=rht>291<td class=rht>3.1%
    <tr><td>amacon.co.jp  (Shift_JIS)<td class=rht>211<td class=rht>216<td class=rht>2.2%
    <tr><td>taobao.com    (gbk)      <td class=rht> 39<td class=rht> 40<td class=rht>1.8%
</table>

<p>Truth of the matter is that to save space a technique independent
  of Unicode should be used.  One that has been around for years and
  any modern browser supports: compression.  And this is also true for
  storage.  Even with a dense ï¬le with virtually no markup
  (e.g. a double newline separating paragraphs as the only ASCII
  characters) it is far better to simply compress the ï¬le then try to
  mess around with encoding.


<h3>There is no Apple logo in Unicode</h3>

<p>Total of 137â€Š468 code points (U+E000â€¦U+F8FF in BMP, U+F0000â€¦U+FFFFD
  in plane 15 and U+100000â€¦U+10FFFD in plane 16) are reserved for
  private use.  In other words, the standard will never assign any
  meaning to them.  If used in a data being interchanged, all parties
  must agree on a common interpretation or else unexpected results
  (maybe even corrupted data) may happen.

<p>This describes situation with Apple logo.  When within realms of
  Cupertino controlled software, U+F8FF is an Apple logo, but outside
  in the world of the free (or at least freer) itâ€™s usually a code
  point with no representation or meaning.

<p>In other words, just donâ€™t use U+F8FF.

<p>Fruit fans should not despair though but rather ï¬nd consolation in
the fact that there is a <span class=udesc>red apple</span> (ğŸ,
U+1F34E), a <span class=udesc>green apple</span> (ğŸ, U+1F34F) and
even a <span class=udesc>pineapple</span> (ğŸ, U+1F34D which isnâ€™t
even an apple nor grows on pine trees).


<h3>Shortening text isnâ€™t quite as easy as you might think</h3>

<p>Speaking of Apple, removing characters from the end of Unicode
  string may expand its rendered representation.  Tom Scott make
  a video about it so rather than duplicating all of his observations,
  Iâ€™ll just point to
  <a href="https://www.youtube.com/watch?v=hJLMSllzoLA">his work on
  the subject</a>.  Itâ€™s not a long video and is worth the watch.


<h3>Conclusion</h3>

<p>There is far more that could be said about Unicode.  Introduction
  of emojis and skin tone modiï¬ers makes the standard so much moreâ€¦
  interesting to name just one aspect.

<p>But even with a limited exposure to the standard this article
  showed Unicode is like localisation: itâ€™s complicated, hard to get
  right and best left to professionals.  And said poor souls, when
  implement Unicode text handling, should remember to
  forget <em>everything</em> they know from other encodings.

<p>Next, accessing code points by index never makes sense.  Code point
  at position <var>n</var> does <em>not</em> correspond
  to <var>n</var>th character nor does it correspond to <var>n</var>th
  glyph.  To take a sub-string of a Unicode text it needs to be
  interpreted from the beginning and having random access to each code
  point doesnâ€™t speed anything up.

<p>And ï¬nally, donâ€™t be fooled by false UTF-16 propaganda.  The encoding
  combines disadvantages of UTF-8 (being variable-length) with
  disadvantages of UTF-32 (taking up a lot of space) and as such is
  the worst possible solution for Unicode text.  Just use UTF-8
  everywhere and be done with it.
