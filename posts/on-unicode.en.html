<!-- subject: On Unicode -->
<!-- date: 2015-10-25 02:11:11 -->
<!-- tags: unicode, utf-8, utf-16, cjk -->
<!-- categories: Articles, Techblog -->

<p>There are <em>aÂ lot</em> of misconceptions about Unicode.  Most are there
  because people assume what they know about ASCII or ISO-8859-* is true about
  Unicode.  They are usually harmless but they tend to creep into minds of
  people who work with text which leads to badly designed software and technical
  decisions made based on false information.

<p>Without further ado, hereâ€™s aÂ few facts about Unicode that might
  surprise you.

<!-- EXCERPT -->

<style>
.hi { text-decoration: overline }
.lo { text-decoration: underline }
.xx { text-decoration: line-through }
.ud { font-variant: all-small-caps }
</style>


<h2>UTF-16 is <em>not</em> aÂ fixed-width encoding</h2>

<p>Unicode defines 17 <dfn>planes</dfn> (most famous being plane zero,
  the Basic Multilingual Plane or BMP).  Each plane consists of
  65â€¯536 <dfn>code points</dfn>.  Quick multiplication unveils
  staggering number of 1â€¯114â€¯112 entries.  It quickly becomes obvious
  that 16 bits, which is the size of aÂ single UTF-16 <dfn>code
  unit</dfn>, arenâ€™t enough to identify each code point uniquely.

<p>To solve that problem, aÂ somehow awkward concept of surrogate pairs
  has been introduced.  (On unrelated note, is it just me who finds
  spelling of â€˜awkwardâ€™ so <em>meta</em>?) Anyway, 2048 code points
  have been carved out to make room for high and low surrogates.  In
  UTF-16, aÂ high surrogate followed by aÂ low surrogateâ€Šâ€”â€Šfour octets
  totalâ€Šâ€”â€Šencodes aÂ single code point outside of BMP.

<p>The encoding method is relatively simple.  For example, to
  represent U+1F574: <span class=ud>man in business suit
  levitating</span> (ğŸ•´â€Šâ€”â€Šdoes your browser support it yet?)
  one would:

<ol>
  <li>
    <p>Subtract 10000â€‰<sub>16</sub> from the code point to produce aÂ 20-bit
      number.
    <p>1F574â€‰<sub>16</sub> - 10000â€‰<sub>16</sub> = F574â€‰<sub>16</sub>
      = <span class=hi>0000111101</span>â€‰<span class=lo>0101110100</span>â€‰<sub>2</sub>
  <li>
    <p>Add D800â€‰<sub>16</sub> to the ten most significant bits of that
      numberâ€Šâ€”â€Šthatâ€™s the high surrogate.
    <p>D800â€‰<sub>16</sub> + <span class=hi>0000111101</span>â€‰<sub>2</sub>
      = 110110â€‰<span class=xx>0000000000</span>â€‰<sub>2</sub>
      + <span class=hi>0000111101</span>â€‰<sub>2</sub>
      = 110110â€‰<span class=hi>0000111101</span>â€‰<sub>2</sub>
      = D83Dâ€‰<sub>16</sub>
  <li>
    <p>Add DC00â€‰<sub>16</sub> to the ten least significant bits of the same
      numberâ€Šâ€”â€Šthatâ€™s the low surrogate.
    <p>DC00â€‰<sub>16</sub> + <span class=lo>0101110100</span>â€‰<sub>2</sub>
      = 110111â€‰<span class=xx>0000000000</span>â€‰<sub>2</sub>
      + <span class=lo>0101110100</span>â€‰<sub>2</sub>
      = 110111â€‰<span class=lo>0101110100</span>â€‰<sub>2</sub>
      = DD74â€‰<sub>16</sub>
  <li>
    <p>Output the high surrogate followed by low surrogate.
    <p>UTF-16 encoding of U+1F574 is U+D83D U+DD74.
</ol>


<h2>Case change is not reversible and may change length</h2>

<p>German speakers probably recognise ÃŸ, aÂ small letter sharp s.  It
  is aÂ bit of aÂ unique snowflake in Latin alphabets in that it has no
  corresponding upper case form.  Or rather, even though capital sharp
  sÂ exists, the correct, according to German orthography, way to
  capitalise ÃŸ is by replacing it with two letters S.  Similarly, an
  ï¬ ligature becomes FI.  Other characters, such as Å‰, need to be
  decomposed producing Ê¼N.

<p>In case this isnâ€™t confusing enough, strings may get shorter as
  well.  â€˜Iâ—ŒÌ‡stanbulâ€™ (which starts with capital IÂ followed by
  U+0307: <span class=ud>combining dot above</span>) becomes
  â€˜istanbulâ€™ when lower cased.  One code point fewer.

<p>Below is aÂ table of some of the corner cases.  Firefox fails at
  Ä°stanbul when spelled using combining dot above character, while
  Chrome and Opera fail to properly capitalise â€˜ï¬lmâ€™.

<table na>
  <thead>
    <tr><th>Operation<th>Expected<th>Browserâ€™s handling
    <tr><th colspan=3>Notes
  <tbody>
    <tr>
      <td scope=row>uc(â€˜deï¬neâ€™)
      <td>DEFINE
      <td style="text-transform:uppercase">deï¬ne
    <tr>
      <td scope=row>uc(â€˜<span lang=de>heiÃŸ</span>â€™)
      <td lang=de>HEISS
      <td style="text-transform:uppercase" lang=de>heiÃŸ
    <tr>
      <td scope=row>tc(â€˜ï¬lmâ€™)
      <td>Film
      <td style="text-transform:capitalize">ï¬lm
    <tr>
      <td colspan=3 style=padding-left:2em>Ligatures and digraphs
        often need to be converted into separate characters.
  <tbody>
    <tr>
      <td scope=row>tc(â€˜<span lang=hr>ÇŒeÅ¾an</span>â€™)
      <td lang=hr>Ç‹eÅ¾an
      <td lang=hr style="text-transform:capitalize">ÇŒeÅ¾an
    <tr>
      <td scope=row>lc(â€˜â…§â€™)
      <td>â…·
      <td style="text-transform:lowercase">â…§
    <tr>
      <td lang=nl scope=row>tc(â€˜Ä³sâ€™)
      <td lang=nl>Ä²s
      <td lang=nl style="text-transform:capitalize">Ä³s
    <tr>
      <td colspan=3 style=padding-left:2em>Some ligatures and digraphs
        have corresponding characters in desired case.
    <tr>
      <td lang=nl scope=row>tc(â€˜ijsâ€™)
      <td lang=nl>IJs
      <td lang=nl style="text-transform:capitalize">ijs
    <tr>
      <td colspan=3 style=padding-left:2em>Interestingly, Firefox
      handles Dutch ij even if written as separate letters.
  <tbody>
   <tr>
      <td scope=row>lc(â€˜<span lang=el>ÎŒÎ£ÎŸÎ£</span>â€™)
      <td lang=el>ÏŒÏƒÎ¿Ï‚</td>
      <td lang=el style="text-transform:lowercase">ÎŒÎ£ÎŸÎ£
    <tr>
      <td colspan=3 style=padding-left:2em>Lower case sigma is â€˜Ïƒâ€™ in
        the middle but â€˜Ï‚â€™ at the end of aÂ word.
  <tbody>
    <tr>
      <td scope=row>uc(â€˜<span lang=tr>istanbul</span>â€™)
      <td lang=tr>Ä°STANBUL</td>
      <td lang=tr style="text-transform:uppercase">istanbul
    <tr>
      <td scope=row>lc(â€˜<span lang=tr>IÌ‡STANBUL</span>â€™)
      <td lang=tr>istanbul</td>
      <td lang=tr style="text-transform:lowercase">IÌ‡STANBUL
    <tr>
      <td scope=row>lc(â€˜<span lang=tr>IRMAK</span>â€™)
      <td lang=tr>Ä±rmak</td>
      <td lang=tr style="text-transform:lowercase">IRMAK
    <tr>
      <td colspan=3 style=padding-left:2em>Turkish has aÂ dot-less
        (a.k.a. closed) and dotted â€˜iâ€™.
</table>


<h2>Single letter may map to multiple code points</h2>

<p>Above examples show that concepts of aÂ letter or aÂ character may be
  blurry and confusing.  Is aforementioned â€˜ÃŸâ€™ aÂ letter or just
  aÂ fancy way of writing â€˜ssâ€™? â€˜szâ€™? What of ligatures and digraphs?
  But at least everyone agrees â€˜Ã©â€™ is aÂ single letter, right?  Here it
  is again: â€˜eÌâ€™, except this time itâ€™s aÂ regular letter eÂ followed by
  U+0301: <span class=ud>combining acute accent</span>, i.e. â€˜eâ—ŒÌâ€™.

<p>The former, single-code-point representation, is
  called <dfn>precomposed</dfn> (or <dfn>composed</dfn>) while the
  latter, using combining characters, is called <dfn>decomposed</dfn>.
  Whatâ€™s important is that both sequences are <dfn>canonically
  equivalent</dfn> and proper Unicode implementations should treat them
  identically.  They should be indistinguishable based on rendering or
  behaviour (e.g. when selecting text).

<p>In addition to the above, Polish â€˜Ä…â€™ â‰ˆ â€˜aâ—ŒÌ¨â€™, Korean â€˜í•œâ€™ â‰ˆ â€˜ã…ã…ã„´â€™,
  â€˜â„¦â€™ (U+2126: <span class=ud>Ohm sign</span>) â‰ˆ â€˜Î©â€™
  (U+03A9: <span class=ud>Greek capital letter omega</span>), Hebrew
  â€˜ï¬«â€â€™ â‰ˆ â€˜×©â€â—Œâ€×‚â€™ and more.

<p>Based on canonical equivalence, Unicode defines aÂ Normalised Form
  CÂ (NFC) and Normalised Form DÂ (NFD).  The former often uses
  precomposed while the latter uses decomposed representation of
  characters.

<p>Oh, and by the way, aforementioned <span class=ud>Ohm
  sign</span> is <dfn>a singleton</dfn> which means that it disappears
  from the text after any kind of normalisation.  Thereâ€™s aÂ bunch of
  those.

<p>â€˜Converting to NFCâ€™, aÂ hopeful programmer will say, â€˜guarantees that aÂ single
  letter maps to no more than one code point!â€™ Alas, noâ€¦ For example, no Unicode
  character for â€˜á¸Ì‡â€™ (i.e. letter dÂ with dot above and below) exists.  No matter
  what form is used, the character must take more than one code point.

<p>NFC is not even guaranteed to be the shortest representation of aÂ given string.  Weâ€™ve already seen that
  â€˜<span dir=ltr>ï¬«</span>â€™ is canonically equivalent to
  â€˜<span dir=ltr>×©â—Œ×‚</span>â€™ but whatâ€™s more interesting is that
  the latter is in NFC.  Yes, even thought precomposed
  character exists, decomposed representation is in NFC.  In fact, for
  <span class=ud>Hebrew letter shin with sin dot</span> normalised
  forms CÂ and DÂ are the same.

<p>As to not leave an impression that NFC is the odd ball here, even
  though NFD usually decomposes precomposed characters, it not always
  does so. â€˜Ã¸â€™ (U+00F8: <span class=ud>Latin small letter oÂ with
  stroke</span>) is in its NFD (as aÂ single code-point) even though
  aÂ decomposed representation with aÂ combining stroke also exists.

<p>Thereâ€™s also aÂ <dfn>compatibility equivalence</dfn> which can be
  thought of as covering â€˜meaningâ€™ of strings.  For example â€˜ï¬â€™
  (U+FB01: <span class=ud>Latin small ligature fi</span>) means the
  same thing as â€˜fÂ + iâ€™, â€˜Ç† âˆ¼ dÂ + zÂ + â—ŒÌŒâ€™ etc.  This is aÂ bit simplified
  view though since â€˜5Â²â€™ has aÂ distinct meaning from â€˜52â€™ yet the
  sequences are in the same compatibly equivalence class.


<h2>UTF-8 is better for CJK than UTF-16</h2>

<p>An argument sometimes put in favour of UTF-16 (over UTF-8) is that
  it is better for far eastern scripts.  For majority of Chinese,
  Japanese and Korean (CJK) ideographs UTF-16 takes just two octets
  while UTF-8 takes three.  Clearly, Asia should abandon UTF-8 and use
  UTF-16 then, right?

<table>
  <thead>
    <tr><th rowspan=2>Block<br>(Range)<th colspan=2>Octets used by
    <tr><th>UTF-8<th>UTF-16
  <tbody>
    <tr><td>CJK Unified Ideographs Extension A<br>(U+3400â€“U+4DBF)  <td>3<td>2
    <tr><td>CJK Unified Ideographs            <br>(U+4E00â€“U+9FFF)  <td>3<td>2
    <tr><td>CJK Unified Ideographs Extension B<br>(U+20000â€“U+2A6DF)<td>4<td>4
    <tr><td>CJK Unified Ideographs Extension C<br>(U+2A700â€“U+2B73F)<td>4<td>4
    <tr><td>CJK Unified Ideographs Extension D<br>(U+2B740â€“U+2B81F)<td>4<td>4
</table>

<p>Alas, the devil, as he often does, lays in the details, namely in
  the fact that in most cases the CJK text is accompanied by markup
  which uses US-ASCII characters.  Since those need only one UTF-8 code
  unit, it often more than makes up for octets â€˜lostâ€™ when encoding
  ideographs.

<p>To see just how big of aÂ role this effect plays in real life
  IÂ looked at aÂ bunch of websites popular in China, Japan and South
  Korea and compared their size (in kibibytes) when different
  encodings were used.  The results are as follows:

<table>
  <thead>
    <tr><th scope=col>Page<th scope=col>UTF-8<th scope=col>UTF-16<th scope=col>Increase
  <tbody>
    <tr><td>baidu.com    <td class=r>   91<td class=r>  181<td class=r>100%
    <tr><td>tmall.com    <td class=r>   46<td class=r>   90<td class=r> 97%
    <tr><td>daum.net     <td class=r>  155<td class=r>  300<td class=r> 94%
    <tr><td>taobao.com   <td class=r>   40<td class=r>   76<td class=r> 93%
    <tr><td>amacon.co.jp <td class=r>  216<td class=r>  413<td class=r> 91%
    <tr><td>rakuten.co.jp<td class=r>  291<td class=r>  548<td class=r> 88%
    <tr><td>gmarket.co.kr<td class=r>   71<td class=r>  133<td class=r> 88%
    <tr><td>weibo.com    <td class=r>    6<td class=r>   11<td class=r> 86%
    <tr><td>yahoo.co.jp  <td class=r>   18<td class=r>   34<td class=r> 85%
    <tr><td>naver.com    <td class=r>   80<td class=r>  147<td class=r> 83%
    <tr><td>ppomppu.co.kr<td class=r>  142<td class=r>  259<td class=r> 83%
    <tr><td>zn.wiki/Japan<td class=r>  938<td class=r>1â€‰690<td class=r> 80%
    <tr><td>kr.wiki/Japan<td class=r>  782<td class=r>1â€‰370<td class=r> 75%
    <tr><td>zn.wiki/Korea<td class=r>   67<td class=r>  116<td class=r> 73%
    <tr><td>fc2.com      <td class=r>   35<td class=r>   60<td class=r> 72%
    <tr><td>jp.wiki/Korea<td class=r>  123<td class=r>  211<td class=r> 71%
    <tr><td>kr.wiki/Korea<td class=r>  180<td class=r>  303<td class=r> 69%
    <tr><td>jp.wiki/Japan<td class=r>1â€‰012<td class=r>1â€‰616<td class=r> 60%
</table>

<p>Yes, in the worst case, baidu.comâ€™s size nearly <em>doubled</em>
  when using UTF-16.

<p>If size is aÂ concern, one might decide to use aÂ dedicated encoding
  such us Shift_JIS, version of EUC or GB2312.  And indeed, some sites
  did that, but even then advantage over UTF-8 was minimal:

<table>
  <thead>
    <tr>
      <th scope=col>Page
      <th scope=col>Original [KiB]
      <th scope=col>UTF-8 [KiB]
      <th scope=col>Increase
  <tbody>
    <tr><td>ppomppu.co.kr (euc-kr)   <td class=r>136<td class=r>142<td class=r>4.5%
    <tr><td>weibo.com     (gb2312)   <td class=r>  5<td class=r>  6<td class=r>3.6%
    <tr><td>gmarket.co.kr (euc-kr)   <td class=r> 69<td class=r> 71<td class=r>3.2%
    <tr><td>rakuten.co.jp (euc-jp)   <td class=r>283<td class=r>291<td class=r>3.1%
    <tr><td>amacon.co.jp  (Shift_JIS)<td class=r>211<td class=r>216<td class=r>2.2%
    <tr><td>taobao.com    (gbk)      <td class=r> 39<td class=r> 40<td class=r>1.8%
</table>

<p>Truth of the matter is that to save space aÂ technique independent
  of Unicode should be used.  One that has been around for years and
  any modern browser supports: compression.  And this is also true for
  storage.  Even with aÂ dense file with virtually no markup
  (e.g. aÂ double newline separating paragraphs as the only ASCII
  characters) it is far better to simply compress the file then try to
  mess around with encoding.


<h2>There is no Apple logo in Unicode</h2>

<p>Total of 137â€‰468 code points (U+E000â€“U+F8FF in BMP, U+F0000â€“U+FFFFD
  in plane 15 and U+100000â€“U+10FFFD in plane 16) are reserved for
  private use.  In other words, the standard will never assign any
  meaning to them.  If used in aÂ data being interchanged, all parties
  must agree on aÂ common interpretation or else unexpected results
  (maybe even corrupted data) may happen.

<p>This describes situation with Apple logo.  When within realms of
  Cupertino controlled software, U+F8FF is an Apple logo, but outside
  in the world of the free (or at least freer) itâ€™s usually aÂ code
  point with no representation or meaning.

<p>In other words, just donâ€™t use U+F8FF.

<p>Fruit fans should not despair though but rather find consolation in
aÂ <span class=ud>red apple</span> (ğŸ, U+1F34E), aÂ <span class=ud>green
apple</span> (ğŸ, U+1F34F) and even aÂ <span class=ud>pineapple</span> (ğŸ,
U+1F34D which isnâ€™t even an apple nor grows on pine trees).


<h2>Shortening text isnâ€™t quite as easy as you might think</h2>

<p>Speaking of Apple, removing characters from the end of Unicode string may
  expand its rendered representation.  Tom Scott made aÂ video about it so rather
  than duplicating all of his observations, Iâ€™ll just point to
  <a href="https://www.youtube.com/watch?v=hJLMSllzoLA">his work on
  the subject</a>.  Itâ€™s not aÂ long video and is worth the watch.


<h2>Conclusion</h2>

<p>There is far more that could be said about Unicode.  Introduction
  of emojis and skin tone modifiers makes the standard so much moreâ€¦
  interesting to name just one aspect.

<p>But even with aÂ limited exposure to the standard this article
  showed Unicode is like localisation: itâ€™s complicated, hard to get
  right and best left to professionals.  And said poor souls, when
  implement Unicode text handling, should remember to
  forget <em>everything</em> they know from other encodings.

<p>Next, accessing code points by index never makes sense.  Code point
  at position <var>n</var> does <em>not</em> correspond
  to <var>n</var>th character nor does it correspond to <var>n</var>th
  glyph.  To take aÂ sub-string of aÂ Unicode text it needs to be
  interpreted from the beginning and having random access to each code
  point doesnâ€™t speed anything up.

<p>And finally, donâ€™t be fooled by false UTF-16 propaganda.  The encoding
  combines disadvantages of UTF-8 (being variable-length) with
  disadvantages of UTF-32 (taking up aÂ lot of space) and as such is
  the worst possible solution for Unicode text.  Just use UTF-8
  everywhere and be done with it.
