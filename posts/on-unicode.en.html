<!-- subject: On Unicode -->
<!-- date: 2015-10-25 02:11:11 -->
<!-- tags: unicode, utf-8, utf-16, cjk -->
<!-- categories: Articles, Techblog -->

<p>There are <em>a lot</em> of misconceptions about Unicode.  Most are there
  because people assume what they know about ASCII or ISO-8859-* is true about
  Unicode.  They are usually harmless but they tend to creep into minds of
  people who work with text which leads to badly designed software and technical
  decisions made based on false information.

<p>Without further ado, here’s a few facts about Unicode that might
  surprise you.

<!-- EXCERPT -->

<style>
.hi { text-decoration: overline }
.lo { text-decoration: underline }
.xx { text-decoration: line-through }
.ud { font-variant: all-small-caps }
</style>


<h2>UTF-16 is <em>not</em> a fixed-width encoding</h2>

<p>Unicode defines 17 <dfn>planes</dfn> (most famous being plane zero,
  the Basic Multilingual Plane or BMP).  Each plane consists of
  65 536 <dfn>code points</dfn>.  Quick multiplication unveils
  staggering number of 1 114 112 entries.  It quickly becomes obvious
  that 16 bits, which is the size of a single UTF-16 <dfn>code
  unit</dfn>, aren’t enough to identify each code point uniquely.

<p>To solve that problem, a somehow awkward concept of surrogate pairs
  has been introduced.  (On unrelated note, is it just me who finds
  spelling of ‘awkward’ so <em>meta</em>?) Anyway, 2048 code points
  have been carved out to make room for high and low surrogates.  In
  UTF-16, a high surrogate followed by a low surrogate — four octets
  total — encodes a single code point outside of BMP.

<p>The encoding method is relatively simple.  For example, to
  represent U+1F574: <span class=ud>man in business suit
  levitating</span> (🕴 — does your browser support it yet?)
  one would:

<ol>
  <li>
    <p>Subtract 10000 <sub>16</sub> from the code point to produce a 20-bit
      number.
    <p>1F574 <sub>16</sub> - 10000 <sub>16</sub> = F574 <sub>16</sub>
      = <span class=hi>0000111101</span> <span class=lo>0101110100</span> <sub>2</sub>
  <li>
    <p>Add D800 <sub>16</sub> to the ten most significant bits of that
      number — that’s the high surrogate.
    <p>D800 <sub>16</sub> + <span class=hi>0000111101</span> <sub>2</sub>
      = 110110 <span class=xx>0000000000</span> <sub>2</sub>
      + <span class=hi>0000111101</span> <sub>2</sub>
      = 110110 <span class=hi>0000111101</span> <sub>2</sub>
      = D83D <sub>16</sub>
  <li>
    <p>Add DC00 <sub>16</sub> to the ten least significant bits of the same
      number — that’s the low surrogate.
    <p>DC00 <sub>16</sub> + <span class=lo>0101110100</span> <sub>2</sub>
      = 110111 <span class=xx>0000000000</span> <sub>2</sub>
      + <span class=lo>0101110100</span> <sub>2</sub>
      = 110111 <span class=lo>0101110100</span> <sub>2</sub>
      = DD74 <sub>16</sub>
  <li>
    <p>Output the high surrogate followed by low surrogate.
    <p>UTF-16 encoding of U+1F574 is U+D83D U+DD74.
</ol>


<h2>Case change is not reversible and may change length</h2>

<p>German speakers probably recognise ß, a small letter sharp s.  It
  is a bit of a unique snowflake in Latin alphabets in that it has no
  corresponding upper case form.  Or rather, even though capital sharp
  s exists, the correct, according to German orthography, way to
  capitalise ß is by replacing it with two letters S.  Similarly, an
  ﬁ ligature becomes FI.  Other characters, such as ŉ, need to be
  decomposed producing ʼN.

<p>In case this isn’t confusing enough, strings may get shorter as
  well.  ‘I◌̇stanbul’ (which starts with capital I followed by
  U+0307: <span class=ud>combining dot above</span>) becomes
  ‘istanbul’ when lower cased.  One code point fewer.

<p>Below is a table of some of the corner cases.  Firefox fails at
  İstanbul when spelled using combining dot above character, while
  Chrome and Opera fail to properly capitalise ‘ﬁlm’.

<table na>
  <thead>
    <tr><th>Operation<th>Expected<th>Browser’s handling
    <tr><th colspan=3>Notes
  <tbody>
    <tr>
      <td scope=row>uc(‘deﬁne’)
      <td>DEFINE
      <td style="text-transform:uppercase">deﬁne
    <tr>
      <td scope=row>uc(‘<span lang=de>heiß</span>’)
      <td lang=de>HEISS
      <td style="text-transform:uppercase" lang=de>heiß
    <tr>
      <td scope=row>tc(‘ﬁlm’)
      <td>Film
      <td style="text-transform:capitalize">ﬁlm
    <tr>
      <td colspan=3 style=padding-left:2em>Ligatures and digraphs
        often need to be converted into separate characters.
  <tbody>
    <tr>
      <td scope=row>tc(‘<span lang=hr>ǌežan</span>’)
      <td lang=hr>ǋežan
      <td lang=hr style="text-transform:capitalize">ǌežan
    <tr>
      <td scope=row>lc(‘Ⅷ’)
      <td>ⅷ
      <td style="text-transform:lowercase">Ⅷ
    <tr>
      <td lang=nl scope=row>tc(‘ĳs’)
      <td lang=nl>Ĳs
      <td lang=nl style="text-transform:capitalize">ĳs
    <tr>
      <td colspan=3 style=padding-left:2em>Some ligatures and digraphs
        have corresponding characters in desired case.
    <tr>
      <td lang=nl scope=row>tc(‘ijs’)
      <td lang=nl>IJs
      <td lang=nl style="text-transform:capitalize">ijs
    <tr>
      <td colspan=3 style=padding-left:2em>Interestingly, Firefox
      handles Dutch ij even if written as separate letters.
  <tbody>
   <tr>
      <td scope=row>lc(‘<span lang=el>ΌΣΟΣ</span>’)
      <td lang=el>όσος</td>
      <td lang=el style="text-transform:lowercase">ΌΣΟΣ
    <tr>
      <td colspan=3 style=padding-left:2em>Lower case sigma is ‘σ’ in
        the middle but ‘ς’ at the end of a word.
  <tbody>
    <tr>
      <td scope=row>uc(‘<span lang=tr>istanbul</span>’)
      <td lang=tr>İSTANBUL</td>
      <td lang=tr style="text-transform:uppercase">istanbul
    <tr>
      <td scope=row>lc(‘<span lang=tr>İSTANBUL</span>’)
      <td lang=tr>istanbul</td>
      <td lang=tr style="text-transform:lowercase">İSTANBUL
    <tr>
      <td scope=row>lc(‘<span lang=tr>IRMAK</span>’)
      <td lang=tr>ırmak</td>
      <td lang=tr style="text-transform:lowercase">IRMAK
    <tr>
      <td colspan=3 style=padding-left:2em>Turkish has a dot-less
        (a.k.a. closed) and dotted ‘i’.
</table>


<h2>Single letter may map to multiple code points</h2>

<p>Above examples show that concepts of a letter or a character may be
  blurry and confusing.  Is aforementioned ‘ß’ a letter or just
  a fancy way of writing ‘ss’? ‘sz’? What of ligatures and digraphs?
  But at least everyone agrees ‘é’ is a single letter, right?  Here it
  is again: ‘é’, except this time it’s a regular letter e followed by
  U+0301: <span class=ud>combining acute accent</span>, i.e. ‘e◌́’.

<p>The former, single-code-point representation, is
  called <dfn>precomposed</dfn> (or <dfn>composed</dfn>) while the
  latter, using combining characters, is called <dfn>decomposed</dfn>.
  What’s important is that both sequences are <dfn>canonically
  equivalent</dfn> and proper Unicode implementations should treat them
  identically.  They should be indistinguishable based on rendering or
  behaviour (e.g. when selecting text).

<p>In addition to the above, Polish ‘ą’ ≈ ‘a◌̨’, Korean ‘한’ ≈ ‘ㅎㅏㄴ’,
  ‘Ω’ (U+2126: <span class=ud>Ohm sign</span>) ≈ ‘Ω’
  (U+03A9: <span class=ud>Greek capital letter omega</span>), Hebrew
  ‘שׂ‎’ ≈ ‘ש‎◌‎ׂ’ and more.

<p>Based on canonical equivalence, Unicode defines a Normalised Form
  C (NFC) and Normalised Form D (NFD).  The former often uses
  precomposed while the latter uses decomposed representation of
  characters.

<p>Oh, and by the way, aforementioned <span class=ud>Ohm
  sign</span> is <dfn>a singleton</dfn> which means that it disappears
  from the text after any kind of normalisation.  There’s a bunch of
  those.

<p>‘Converting to NFC’, a hopeful programmer will say, ‘guarantees that a single
  letter maps to no more than one code point!’ Alas, no… For example, no Unicode
  character for ‘ḍ̇’ (i.e. letter d with dot above and below) exists.  No matter
  what form is used, the character must take more than one code point.

<p>NFC is not even guaranteed to be the shortest representation of a given string.  We’ve already seen that
  ‘<span dir=ltr>שׂ</span>’ is canonically equivalent to
  ‘<span dir=ltr>ש◌ׂ</span>’ but what’s more interesting is that
  the latter is in NFC.  Yes, even thought precomposed
  character exists, decomposed representation is in NFC.  In fact, for
  <span class=ud>Hebrew letter shin with sin dot</span> normalised
  forms C and D are the same.

<p>As to not leave an impression that NFC is the odd ball here, even
  though NFD usually decomposes precomposed characters, it not always
  does so. ‘ø’ (U+00F8: <span class=ud>Latin small letter o with
  stroke</span>) is in its NFD (as a single code-point) even though
  a decomposed representation with a combining stroke also exists.

<p>There’s also a <dfn>compatibility equivalence</dfn> which can be
  thought of as covering ‘meaning’ of strings.  For example ‘ﬁ’
  (U+FB01: <span class=ud>Latin small ligature fi</span>) means the
  same thing as ‘f + i’, ‘ǆ ∼ d + z + ◌̌’ etc.  This is a bit simplified
  view though since ‘5²’ has a distinct meaning from ‘52’ yet the
  sequences are in the same compatibly equivalence class.


<h2>UTF-8 is better for CJK than UTF-16</h2>

<p>An argument sometimes put in favour of UTF-16 (over UTF-8) is that
  it is better for far eastern scripts.  For majority of Chinese,
  Japanese and Korean (CJK) ideographs UTF-16 takes just two octets
  while UTF-8 takes three.  Clearly, Asia should abandon UTF-8 and use
  UTF-16 then, right?

<table>
  <thead>
    <tr><th rowspan=2>Block<br>(Range)<th colspan=2>Octets used by
    <tr><th>UTF-8<th>UTF-16
  <tbody>
    <tr><td>CJK Unified Ideographs Extension A<br>(U+3400–U+4DBF)  <td>3<td>2
    <tr><td>CJK Unified Ideographs            <br>(U+4E00–U+9FFF)  <td>3<td>2
    <tr><td>CJK Unified Ideographs Extension B<br>(U+20000–U+2A6DF)<td>4<td>4
    <tr><td>CJK Unified Ideographs Extension C<br>(U+2A700–U+2B73F)<td>4<td>4
    <tr><td>CJK Unified Ideographs Extension D<br>(U+2B740–U+2B81F)<td>4<td>4
</table>

<p>Alas, the devil, as he often does, lays in the details, namely in
  the fact that in most cases the CJK text is accompanied by markup
  which uses US-ASCII characters.  Since those need only one UTF-8 code
  unit, it often more than makes up for octets ‘lost’ when encoding
  ideographs.

<p>To see just how big of a role this effect plays in real life
  I looked at a bunch of websites popular in China, Japan and South
  Korea and compared their size (in kibibytes) when different
  encodings were used.  The results are as follows:

<table>
  <thead>
    <tr><th scope=col>Page<th scope=col>UTF-8<th scope=col>UTF-16<th scope=col>Increase
  <tbody>
    <tr><td>baidu.com    <td class=r>   91<td class=r>  181<td class=r>100%
    <tr><td>tmall.com    <td class=r>   46<td class=r>   90<td class=r> 97%
    <tr><td>daum.net     <td class=r>  155<td class=r>  300<td class=r> 94%
    <tr><td>taobao.com   <td class=r>   40<td class=r>   76<td class=r> 93%
    <tr><td>amacon.co.jp <td class=r>  216<td class=r>  413<td class=r> 91%
    <tr><td>rakuten.co.jp<td class=r>  291<td class=r>  548<td class=r> 88%
    <tr><td>gmarket.co.kr<td class=r>   71<td class=r>  133<td class=r> 88%
    <tr><td>weibo.com    <td class=r>    6<td class=r>   11<td class=r> 86%
    <tr><td>yahoo.co.jp  <td class=r>   18<td class=r>   34<td class=r> 85%
    <tr><td>naver.com    <td class=r>   80<td class=r>  147<td class=r> 83%
    <tr><td>ppomppu.co.kr<td class=r>  142<td class=r>  259<td class=r> 83%
    <tr><td>zn.wiki/Japan<td class=r>  938<td class=r>1 690<td class=r> 80%
    <tr><td>kr.wiki/Japan<td class=r>  782<td class=r>1 370<td class=r> 75%
    <tr><td>zn.wiki/Korea<td class=r>   67<td class=r>  116<td class=r> 73%
    <tr><td>fc2.com      <td class=r>   35<td class=r>   60<td class=r> 72%
    <tr><td>jp.wiki/Korea<td class=r>  123<td class=r>  211<td class=r> 71%
    <tr><td>kr.wiki/Korea<td class=r>  180<td class=r>  303<td class=r> 69%
    <tr><td>jp.wiki/Japan<td class=r>1 012<td class=r>1 616<td class=r> 60%
</table>

<p>Yes, in the worst case, baidu.com’s size nearly <em>doubled</em>
  when using UTF-16.

<p>If size is a concern, one might decide to use a dedicated encoding
  such us Shift_JIS, version of EUC or GB2312.  And indeed, some sites
  did that, but even then advantage over UTF-8 was minimal:

<table>
  <thead>
    <tr>
      <th scope=col>Page
      <th scope=col>Original [KiB]
      <th scope=col>UTF-8 [KiB]
      <th scope=col>Increase
  <tbody>
    <tr><td>ppomppu.co.kr (euc-kr)   <td class=r>136<td class=r>142<td class=r>4.5%
    <tr><td>weibo.com     (gb2312)   <td class=r>  5<td class=r>  6<td class=r>3.6%
    <tr><td>gmarket.co.kr (euc-kr)   <td class=r> 69<td class=r> 71<td class=r>3.2%
    <tr><td>rakuten.co.jp (euc-jp)   <td class=r>283<td class=r>291<td class=r>3.1%
    <tr><td>amacon.co.jp  (Shift_JIS)<td class=r>211<td class=r>216<td class=r>2.2%
    <tr><td>taobao.com    (gbk)      <td class=r> 39<td class=r> 40<td class=r>1.8%
</table>

<p>Truth of the matter is that to save space a technique independent
  of Unicode should be used.  One that has been around for years and
  any modern browser supports: compression.  And this is also true for
  storage.  Even with a dense file with virtually no markup
  (e.g. a double newline separating paragraphs as the only ASCII
  characters) it is far better to simply compress the file then try to
  mess around with encoding.


<h2>There is no Apple logo in Unicode</h2>

<p>Total of 137 468 code points (U+E000–U+F8FF in BMP, U+F0000–U+FFFFD
  in plane 15 and U+100000–U+10FFFD in plane 16) are reserved for
  private use.  In other words, the standard will never assign any
  meaning to them.  If used in a data being interchanged, all parties
  must agree on a common interpretation or else unexpected results
  (maybe even corrupted data) may happen.

<p>This describes situation with Apple logo.  When within realms of
  Cupertino controlled software, U+F8FF is an Apple logo, but outside
  in the world of the free (or at least freer) it’s usually a code
  point with no representation or meaning.

<p>In other words, just don’t use U+F8FF.

<p>Fruit fans should not despair though but rather find consolation in
a <span class=ud>red apple</span> (🍎, U+1F34E), a <span class=ud>green
apple</span> (🍏, U+1F34F) and even a <span class=ud>pineapple</span> (🍍,
U+1F34D which isn’t even an apple nor grows on pine trees).


<h2>Shortening text isn’t quite as easy as you might think</h2>

<p>Speaking of Apple, removing characters from the end of Unicode string may
  expand its rendered representation.  Tom Scott made a video about it so rather
  than duplicating all of his observations, I’ll just point to
  <a href="https://www.youtube.com/watch?v=hJLMSllzoLA">his work on
  the subject</a>.  It’s not a long video and is worth the watch.


<h2>Conclusion</h2>

<p>There is far more that could be said about Unicode.  Introduction
  of emojis and skin tone modifiers makes the standard so much more…
  interesting to name just one aspect.

<p>But even with a limited exposure to the standard this article
  showed Unicode is like localisation: it’s complicated, hard to get
  right and best left to professionals.  And said poor souls, when
  implement Unicode text handling, should remember to
  forget <em>everything</em> they know from other encodings.

<p>Next, accessing code points by index never makes sense.  Code point
  at position <var>n</var> does <em>not</em> correspond
  to <var>n</var>th character nor does it correspond to <var>n</var>th
  glyph.  To take a sub-string of a Unicode text it needs to be
  interpreted from the beginning and having random access to each code
  point doesn’t speed anything up.

<p>And finally, don’t be fooled by false UTF-16 propaganda.  The encoding
  combines disadvantages of UTF-8 (being variable-length) with
  disadvantages of UTF-32 (taking up a lot of space) and as such is
  the worst possible solution for Unicode text.  Just use UTF-8
  everywhere and be done with it.
